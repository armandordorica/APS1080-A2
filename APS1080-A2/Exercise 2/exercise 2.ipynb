{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fcad244-f231-469f-80f4-0b3fce6e0459",
   "metadata": {},
   "source": [
    "Open Files/A2_... and study it in Colab by running it.\n",
    "\n",
    "Observe how all the facets of a reinforcement learning coupled machine/environment system are present.\n",
    "\n",
    "The notebook includes some code to show how the behaviour of the agent can be rendered, using a random policy that exploits the .sample() method.\n",
    "\n",
    "Exercise 1:\n",
    "\n",
    "Can you design a dynamic programming based policy for the agent as in assignment 1? If so, design it and demonstrate that it solves the cart pole problem.\n",
    "\n",
    "Exercise 2:\n",
    "\n",
    "Can you design a Monte Carlo based policy for the agent? What ingredients do you require? Explain the design flow, and execute it. Show that it works, or indicate why you can't proceed.\n",
    "\n",
    " \n",
    "Submission:\n",
    "\n",
    "Submit a pdf containing your answers, including any code you've written.\n",
    "\n",
    " \n",
    "\n",
    "This exercise should take one or two days, however, you'll be given a week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7413a5f2-493a-4885-81f9-651fd4a4482a",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 1:\n",
    "\n",
    "Can you design a dynamic programming based policy for the agent as in assignment 1? If so, design it and demonstrate that it solves the cart pole problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4571c6-e474-43b2-83de-c34d911c217a",
   "metadata": {},
   "source": [
    "The state of the cart-pole problem is continuous, which means that traditional methods like Dynamic Programming are not very suitable. The way around it would be to discretize the continous space. This can be accomplished by binning the continuous states, thereby creating a \"grid\" of states, similar to the gridworld problem. \n",
    "\n",
    "However, as you can see, the process can be quite slow because the number of possible states after discretization is still very large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b09d128-1c7f-4195-a7cf-9c08b47b5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Define the environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Discretization: We will represent states as discrete values rather than a continuous range.\n",
    "# We'll use 10 bins for each of the four state variables (cart position, cart velocity, pole angle, pole velocity at tip).\n",
    "NUM_BINS = [10, 10, 10, 10]  \n",
    "bins = [np.linspace(-0.5, 0.5, num) for num in NUM_BINS]\n",
    "\n",
    "# Initialize value table to zeros\n",
    "values = np.zeros(NUM_BINS + [env.action_space.n])\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.95\n",
    "\n",
    "# Policy: At each state, the agent will choose the action with the highest expected future reward.\n",
    "policy = np.zeros(NUM_BINS, dtype=int)\n",
    "\n",
    "def discretize(state):\n",
    "    \"\"\"Convert continuous state into discrete bins.\"\"\"\n",
    "    return tuple(np.digitize(s, bins[i]) - 1 for i, s in enumerate(state))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d0e611d-bdb6-4319-91d3-2c198e92e495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m (next_state, reward, terminated, truncated, info) \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# next_state, reward, _, _ = env.step(action)  # Take the action\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[43mdiscretize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Bellman equation for value iteration\u001b[39;00m\n\u001b[1;32m     12\u001b[0m new_values[state][action] \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(values[next_state])\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mdiscretize\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdiscretize\u001b[39m(state):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124;03m\"\"\"Convert continuous state into discrete bins.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdigitize\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdiscretize\u001b[39m(state):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124;03m\"\"\"Convert continuous state into discrete bins.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdigitize\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(state))\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdigitize\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:5507\u001b[0m, in \u001b[0;36mdigitize\u001b[0;34m(x, bins, right)\u001b[0m\n\u001b[1;32m   5505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bins) \u001b[38;5;241m-\u001b[39m _nx\u001b[38;5;241m.\u001b[39msearchsorted(bins[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], x, side\u001b[38;5;241m=\u001b[39mside)\n\u001b[1;32m   5506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 5507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msearchsorted\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:1387\u001b[0m, in \u001b[0;36msearchsorted\u001b[0;34m(a, v, side, sorter)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearchsorted\u001b[39m(a, v, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, sorter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;124;03m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1385\u001b[0m \n\u001b[1;32m   1386\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearchsorted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:51\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     47\u001b[0m         result \u001b[38;5;241m=\u001b[39m wrap(result)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapfunc\u001b[39m(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     52\u001b[0m     bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(1000):  # Iterate\n",
    "    print(_)\n",
    "    new_values = np.copy(values)  # Copy current value table\n",
    "    for state in np.ndindex(*NUM_BINS):  # For each state in the state space\n",
    "        for action in range(env.action_space.n):  # For each action\n",
    "            env.reset()\n",
    "            env.env.state = state\n",
    "            (next_state, reward, terminated, truncated, info) = env.step(action)\n",
    "            # next_state, reward, _, _ = env.step(action)  # Take the action\n",
    "            next_state = discretize(next_state)\n",
    "            # Bellman equation for value iteration\n",
    "            new_values[state][action] = reward + gamma * np.max(values[next_state])\n",
    "    values = new_values  # Update value table\n",
    "\n",
    "# Update policy\n",
    "for state in np.ndindex(*NUM_BINS):\n",
    "    policy[state] = np.argmax(values[state])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5eb9e6-ec9f-47b9-986a-e0c305570b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "learning_rate = 0.01\n",
    "episodes = 1000\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=4, activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "\n",
    "# Train model\n",
    "for episode in range(episodes):\n",
    "    state = env.reset().reshape(1, 4)\n",
    "    done = False\n",
    "    time = 0\n",
    "\n",
    "    while not done:\n",
    "        time += 1\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(2)\n",
    "        else:\n",
    "            action = np.argmax(model.predict(state))\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        next_state = next_state.reshape(1, 4)\n",
    "        target = reward + gamma * np.amax(model.predict(next_state)[0])\n",
    "        target_f = model.predict(state)\n",
    "        target_f[0][action] = target\n",
    "        model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print(\"Episode: {}/{}, Score: {}\".format(episode, episodes, time))\n",
    "            break\n",
    "\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25888858-1b2b-4272-b953-a31beeb3b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49423a9-28a4-4c25-b3cc-14846b8f53b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
